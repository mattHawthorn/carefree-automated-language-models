#coding: utf-8from .corpus import Vocabulary, BagOfWordsCorpusfrom .objects import *from .processor import ngramIterfrom .utils import getsizefrom numpy import arrayimport numpy as npimport os, random, sysfrom itertools import chain,repeatfrom math import log, expfrom random import random, sample################################################### NGRAM LANGUAGE MODELS ############################################################################def additiveSmoothingP(ngram,model,param=0.1):    if len(ngram) == 0:        return 1.0        token = ngram[-1]    ngram = ngram[0:-1]    try:        node = model.node(ngram)    except:        return param/float(model.V)    else:        try:            count = node[token].count + param        except:            count = param        return float(count)/(param*model.V + node.count)            def absoluteDiscountP(ngram,model,param=0.1):    if len(ngram) == 0:        return 1.0            token = ngram[-1]        if len(ngram) == 1:        if token in model.root:            count = float(model.root[token].count)        else:            count = min(1.0,model.new)        return count/(model.root.count + model.new)        prefix = ngram[0:-1]    try:        node = model.node(prefix)    except:        # no tokens seen past the prefix; fall back on the suffix distribution        return absoluteDiscountP(ngram[1:],model,param)    else:        try:            count = float(node[token].count - param)        except:            if token in model.vocab.ID:                count = 0.0            else:                count = min(1.0,model.new)                total = float(node.count + model.new)        c = float(param * node.unique)/total        return count/total + c * absoluteDiscountP(ngram[1:],model,param)def kneserNeyP(ngram,model,param=0.1):    if len(ngram) == 0:        return 1.0            token = ngram[-1]        if len(ngram) == 1:        if token in model.root:            count = float(model.root[token].count)        else:            count = min(1.0,model.new)        return count/(model.root.count + model.new)        prefix = ngram[0:-1]    try:        node = model.node(prefix)    except:        # no tokens seen past the prefix; fall back on the suffix distribution        return absoluteDiscountP(ngram[1:],model,param)    else:        try:            count = float(node[token].count - param)        except:            if token in model.vocab.ID:                count = 0.0            else:                count = min(1.0,model.new)                total = float(node.count + model.new)        c = float(param * node.unique)/total        return count/total + c * absoluteDiscountP(ngram[1:],model,param)smoothingMethods = {    'additive': additiveSmoothingP,    'laplace': additiveSmoothingP,    'linear interpolation': absoluteDiscountP,    'kneser ney': kneserNeyP}################################################### OLD - REVISE #####################################################################################        class NodeProbModel:    """    Probability model for a single node of an Ngram language model probability trie.    No local information regarding smoothing is stored here, but rather in the parent model, which passes that information for    initialization.    Has a list of cumsum'ed probabilities for fast sampling via binary search, and a correspoding list of token IDs to sample from.    Also keeps track of reserved probability for interpolation (in the case of linear and absolute discount) or unseen tokens     (in the case of additive smoothing)    """    __slots__=('probs','ID','reserved','unseen')    def __init__(self,freqTrieNode,V,smoothing,param,keyDtype='uint32'):        # total unique tokens seen at the node        unique = freqTrieNode.unique        # total token count seen at the node        count = freqTrieNode.count        # total unique unseen tokens at the node        unseen = V - unique                # ID's of all tokens seen at the node stored as an array (uint32 used to save space over int64 default,         # 16 will do for most corpora)        IDs = np.array(list(freqTrieNode.keys()),dtype=keyDtype)        # stored as an easily searchable (sorted) array for purposes of lookup        IDs = np.sort(IDs)                # intialize probabilities as counts        probs = np.array([freqTrieNode[int(ID)].count for ID in IDs],dtype='float32')                # apply smoothing depending on type        if smoothing == 'additive':            # total smoothed count of tokens            total = count + param*V            # add the constant delta to the counts and normalize            probs = (probs + param)/float(total)            # probability mass reserved for unseen tokens            self.reserved = param*unseen/total            if unseen == 0:                self.unseen = 0            else:                self.unseen = self.reserved/unseen                    elif smoothing == 'linear interpolation':            # simply take the ML estimate, multiplied by lambda            probs = probs*param/count            # then set the reserved probability to 1-lambda            self.reserved = 1 - param                    elif smoothing == 'absolute discount':            # subtract the constant delta from the counts and normalize            probs = (probs - param)/V            # the probability mass reserved for unseen tokens            self.reserved = param*unique/V                    # store the probabilities and IDs        self.probs = np.cumsum(probs)        self.ID = IDs            def sample(self):        # take the cumsum for sampling purposes; this can be binary-searched for efficient sampling        probs = np.cumsum(self.probs)                # sample from Unif(0,1)        rand = random()        # get the index of the entry just greater than rand in the cumsum probs array using binary search        index = binarySearch(rand,probs)                # if this returned an index, then get the ID of the token        if index:            ID = self.ID[index]        # otherwise, rand was greater than the greatest cumsum prob; this occurs with a probability of self.reserved        # I return None and defer to the parent trie for the appropriate sampling procedure, which depends on the        # global vocab and the smoothing type        else:            ID = None                    return IDclass NgramModel:    """    Ngram language model.  Modelled as a trie with transition probabilities at each node.    Methods for computation of probability and perplexity of a token sequence, as well as    random text generation.  Training is assumed to be in batch mode on a complete previously    generated frequency trie.  In this way, probability calculations and sampling can be    performed quickly.    """        def __init__(self,n,smoothing='additive',param=1,newTokens=1,keyDtype='uint32'):        """        Smoothing is one of: {"absolute discount","linear interpolation","additive"}        param is the single parameter supplied to each of these methods.  Traditionally called delta in the additive         and discount cases, and lambda in the linear case.  lambda is a keyword in python, so I choose simply 'param'         to capture all cases.        newTokens is a number which allows allocation of probability mass to tokens not in the vocab, i.e. for        calculating perplexity on documents outside the training set.  This is added to the vocab size self.V, and         propagates to all computed probabilites in interpolation calculations.        """                # smoothing type        self.smoothing = smoothing        # the smoothing parameter        self.param = param        # the numpy datatype of the token keys, for use in the generative models.          # This can save considerable space over the standard 64-bit int; an unsigned 16-bit         # int can for example index a vocublary of size 65535 or less.        self.key_dtype=keyDtype        # empty root node        self.root = None        self.vocab = None        self.V = None        self.n = n        self.new = newTokens            def trainOnDocs(self,docs):        """        Rather than try to infer anything about the structure of the docs, we simply assume that         docs is an iterable of tokenized documents, i.e. an iterable of iterables of tokens.        """        freqTrie = FreqTrie(self.n)                # Determine how to iterate over docs; if list, tuple, or set, do nothing        #if type(docs) is dict:        #    docs = docs.values()        #elif type(docs) is BagOfWordsCorpus:        #    if type(docs.docs) is list:        #        docs = docs.docs        #    elif type(docs.docs) is dict:        #        docs = docs.docs.values()        # At this point if the type of docs hasn't been caught, we assume it's an iterable of        # calm.corpus.Document objects                    for doc in docs:            ngrams = ngramIter(doc,self.n)            for ngram in ngrams:                freqTrie.add(ngram)                self.trainOnTrie(freqTrie)                def trainOnTrie(self,freqTrie):        # object is of either the FreqTrie class, and is appended at the root node of the probability        # model, or is of the BagOfWordsCorpus class.  nodes are recursively assigned probabilities and generative models.                if freqTrie.n < self.n:            raise ValueError("This model must be trained on a trie of depth {0}.".format(self.n))            return                # the vocab        self.vocab = freqTrie.vocab        self.V = self.vocab.size + self.new        # establish the root node        self.root = freqTrie.root        # build the sampling models recursively on the root node        # these do not take interpolation into account, but allows sampling random         # sequences of tokens via tracking of reserved probability mass for         # unseen tokens or allocation to the suffix (n-1)-gram in interpolation.        # self.__conditional(ngram) is computed on the fly and takes interpolation into account        self.__makeModel(self.root)                    def __makeModel(self,node):        # make the sampling model for the node        node.model = NodeProbModel(freqTrieNode=node,V=self.vocab.size,smoothing=self.smoothing,                                   param=self.param,keyDtype=self.key_dtype)        # this model has a 'reserved' probability, which in the case of absolute discount or        # linear interpolation, is the probability mass given to the distribution on the prefix        # node, i.e. p(w3|w1w2) = p(w3 at node w1w2 from counts at node w1w2) + reserved*p(w2|w1).        # in the case of additive smoothing, this is just the probability distributed evenly over        # unseen tokens.                for child in node.values():            self.__makeModel(node=child)                                def p(self,tokens,logarithm=False):        # return probability of an iterable of tokens        # use the log-sum-exp to avoid over/underflow, optionally returning log-probability                tokens = self.IDs(tokens)        logp = 0                # first, the log probability of the head of the list (1-gram, 2-gram, ... (n-1)-gram)        l = min(self.n - 1, len(tokens))                for i in range(1,l+1):            ngram = tokens[0:i]            logp += log(self.__conditional(ngram))                # now, the rest of the tokens        if len(tokens) >= self.n:            for ngram in ngramIter(tokens,self.n):                logp += log(self.__conditional(ngram))                        if logarithm:            return logp        else:            return exp(logp)        def perplexity(self,tokens,logarithm=False):        # get the perplexity of a sequence of tokens        logp = self.p(tokens,logarithm=True)        logp = -1*logp/len(tokens)                if logarithm:            return logp        else:            return(exp(logp))            def conditional(self,ngram):        # for the user interface; can give raw tokens rather than their int IDs        IDs = self.IDs(ngram)        return self.__conditional(IDs)            def __conditional(self,ngram):        # __conditional probability of the last word in an ngram given the prior word        # all tokens are assumed to be their int ID's here.                P = smoothingFunctions[self.smoothing]        return P(ngram,self,self.param)            def node(self,IDs):        node = self.root        for ID in IDs:            node = node[ID]        return node        def __getitem__(self,ngram):        IDs = self.IDs(ngram)        if None in IDs:            raise KeyError("node does't exist")        else:            return self.node(IDs)        def topTokens(self,ngram,k=10):        node = self[ngram]        IDs = self.IDs(ngram)        probs = [(ID,self.__conditional(IDs + [ID])) for ID in self.vocab.token]        probs = sorted(probs,key= (lambda x: x[1]), reverse=True)        probs = probs[0:k]        topk = [(self.vocab.token[ID],p) for ID,p in probs]        return topk        def IDs(self,tokens):        # return a list of token IDs from a list of tokens (filling None where missing from the vocab)        length = len(tokens)        # initialize a list        IDs = [0]*length        # get the token IDs        for i in range(0,length):            try:                IDs[i] = self.vocab.ID[tokens[i]]            except KeyError:                IDs[i] = None                return IDs            def generate(self,maxLength,beginTokens=None,endToken=None):        tokens = list()        length = 0                if beginTokens:            prior = self.IDs(beginTokens)        else:            prior = []                while length < maxLength:            # sample an ID            ID = None            newprior = prior                        while not ID:                try:                    node = self.node(newprior)                except:                    # prior ngram doesn't exist; in the case of additive, we would                    # sample from the vocab evenly:                    if self.smoothing == 'additive':                        ID = sample(list(self.vocab.token),1)[0]                                            # in the interpolation case, sample from the suffix node.                    # this happens with the correct probability assured by the                     # construction of the model.                    elif self.smoothing in ['absolute discount','linear interpolation']:                        if len(newprior) == 1:                            newprior == []                        else:                            newprior = newprior[1:len(newprior)]                else:                    ID = node.model.sample()                    if not ID:                        if self.smoothing == 'additive':                            # sample from the unseen tokens evenly in the additive case                            ID = sample(set(self.vocab.token).difference(set(node)),1)[0]                            print(ID)                            break                        if len(newprior) == 0:                            # we've sampled from probability not seen at the root;                            if self.smoothing in ['absolute discount','linear interpolation']:                                # sample evenly from the vocab in the interpolation case (0-gram distribution)                                ID = sample(list(self.vocab.token),1)[0]                                                        elif len(newprior) == 1:                            newprior == []                        else:                            newprior = newprior[1:len(newprior)]                        token = self.vocab.token[ID]            tokens.append(ID)            l = min(self.n,len(tokens))            prior = tokens[-l:len(tokens)]            length += 1                        if endToken and token == endToken:                break                    tokens = [self.vocab.token[ID] for ID in tokens]        return tokens        